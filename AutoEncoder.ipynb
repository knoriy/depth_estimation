{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReCoNet_AutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNmzHENpkTPg3hu0uJ7UhUr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knoriy/depth_estimation/blob/master/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUF3878v3gAB",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOljj91bhJcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Add, Activation, BatchNormalization, Conv2D, Conv2DTranspose, AveragePooling2D, MaxPooling2D, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn4xE7Fbh9Fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import tqdm\n",
        "import shutil\n",
        "import imageio\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-HSEiBEqPRd",
        "colab_type": "text"
      },
      "source": [
        "## Globals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeHkmeNfqQ8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_weight            = 1e-1  # Weight, Controling the contribution of the style image\n",
        "content_weight          = 0.01   # Weight, Controling the contribution of the content image\n",
        "total_variation_weight  = 1e4   # Weight, controling the total change across the image (removing noise)\n",
        "\n",
        "epochs                  = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdKK1x67c58X",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geSPs437dEqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Y-zn8gf846",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_to_image( tensor ):\n",
        "  '''\n",
        "  This function converts a tensor to a numpy array that can be read by PIL \n",
        "\n",
        "  Arguments:\n",
        "    tensor - A tensorflow tensor\n",
        "\n",
        "  Returns:\n",
        "    A PIL image\n",
        "  '''\n",
        "  tensor = tensor*255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor)>3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return PIL.Image.fromarray(tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxomQJcohqe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_time(): \n",
        "  '''\n",
        "  This function gets the current date and time (d-m-y-h-m-s)\n",
        "\n",
        "  Return:\n",
        "    String, Containing the day, month, year, hour, minute, seconds\n",
        "  '''\n",
        "  return time.strftime(\"%d-%m-%y_%H-%M-%S\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXnccf5wiQlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_files(*directories):\n",
        "  '''\n",
        "  This functions deletes files or directories passed in.\n",
        "\n",
        "  Arguments:\n",
        "    *directory - String, The directory or file to be removed. This is a *args argument, any number or \n",
        "                 arguments can be passed and the delete operation will be performed of what is passed\n",
        "  '''\n",
        "  for directory in directories:\n",
        "    if not os.path.exists(directory):\n",
        "      raise ValueError(\"{} could not be found, please check it exists\".format(file_name))\n",
        "\n",
        "  for directory in tqdm.tqdm(directories):\n",
        "    if os.path.isdir(directory):\n",
        "      shutil.rmtree(directory)\n",
        "    elif os.path.isfile(directory):\n",
        "      os.remove(directory)\n",
        "    else:\n",
        "      raise OSError(\"UNKOWN ERROR: Could not delete {}\".format(directory))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vML36FFrSkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_video(save_dir, source_dir, mode=\"?\"):\n",
        "  '''\n",
        "  This function created a video from a given list of images\n",
        "\n",
        "  Arguments:\n",
        "    save_dir    - Takes in as string, The directory to save the video\n",
        "    source_dir  - Takes in as string, The directory to the images to be convereted to video\n",
        "    mode        - Used to give the writer a hint on what the user expects (default ‘?’): “i” for an image, \n",
        "                  “I” for multiple images, “v” for a volume, “V” for multiple volumes, “?” for don’t care.\n",
        "  \n",
        "  '''\n",
        "  # creating a abs list of images directories in a given dirctory\n",
        "  source_images = [os.path.join(source_dir, file) for file in os.listdir(source_dir)]\n",
        "\n",
        "  with imageio.get_writer(save_dir, mode=mode) as writer:\n",
        "    for source_image in tqdm.tqdm(source_images):\n",
        "      image = imageio.imread(source_image)\n",
        "      writer.append_data(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU9KXjXJV1wj",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbVKpyMm42_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img( path_to_image ):\n",
        "  '''\n",
        "    This function loads a given image and normalized this into [0,1] range\n",
        "\n",
        "    Arguments:\n",
        "      path_to_img - expects string to the file\n",
        "\n",
        "    Returns:\n",
        "      A converted tenor with a range of [0,1]\n",
        "\n",
        "  '''\n",
        "  img = tf.io.read_file(path_to_image)                # load the raw data from the file as a string\n",
        "  img = tf.image.decode_image(img, channels=3)        # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32) # convert image from 0-255 to floats in the [0,1] range.\n",
        "\n",
        "  # img = img[tf.newaxis, :]\n",
        "\n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8O1hvzZ47is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################\n",
        "# Resizing image, is broken, due to me adding \"img = img[tf.newaxis, :]\" into load_image\n",
        "def image_resize( image, max_dim ):\n",
        "  '''\n",
        "    Resizes an given image\n",
        "\n",
        "    Arguments:\n",
        "      image     - The image to be resized\n",
        "      max_dim - The maximum dimension in a given axis\n",
        "\n",
        "    Returns:\n",
        "     A resized image in tensor\n",
        "    \n",
        "  '''\n",
        "\n",
        "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "  image = image[tf.newaxis, :]\n",
        "  return image\n",
        "\n",
        "def image_resize_with_crop_or_pad( image, target_height, target_width):\n",
        "  image = tf.image.resize_with_crop_or_pad(image, target_height, target_width) \n",
        "  image = image[tf.newaxis, :]\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m28hdBr94-VM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  '''\n",
        "    Display a given image on screen\n",
        "\n",
        "    Arguments:\n",
        "      image - The image to be displayed\n",
        "      title - expects string, the tile to be displayed above the image\n",
        "\n",
        "  '''\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7YK1BwvWL0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getImageList(dir):\n",
        "  '''\n",
        "  This fuction gets a list of all files in a given directory\n",
        "  \n",
        "  Arguments:\n",
        "    dir - The directory to the files\n",
        "  \n",
        "  Returns: \n",
        "    A list of files\n",
        "\n",
        "  '''\n",
        "  contents = os.listdir(dir)\n",
        "\n",
        "  dir_list = []\n",
        "  for content in contents:\n",
        "    dir_list.append(os.path.join(dir, content))\n",
        "\n",
        "  return dir_list\n",
        "\n",
        "def getImageSequence(images):\n",
        "  '''\n",
        "  This function creates a sequence of decoded images in [0,1] space.\n",
        "\n",
        "  Arguments:\n",
        "    images - Expects a list of directories.\n",
        "\n",
        "  Returns:\n",
        "    A list of converted tenors with a range of [0,1].\n",
        "    \n",
        "  '''\n",
        "\n",
        "  tf_images = []\n",
        "  for image in images:\n",
        "    tf_images.append(load_img(image))\n",
        "  return tf_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve49EJiqtDwP",
        "colab_type": "text"
      },
      "source": [
        "# MODEL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LVC3V9Bm5Np",
        "colab_type": "text"
      },
      "source": [
        "## AutoEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OfBd9w14RLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(x, filters, kernel_size, stride, padding='same'):\n",
        "  '''\n",
        "    Implementation of the identity block as shown in the ResNet paper:\n",
        "      https://arxiv.org/abs/1512.03385\n",
        "    \n",
        "    Arguments:\n",
        "      x           - Input tensor of the previous layers \n",
        "      filters     - Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "      kernel_size - An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
        "                    Can be a single integer to specify the same value for all spatial dimensions.\n",
        "      stride      - An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and \n",
        "                    width. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "      padding     - one of \"valid\" or \"same\" (case-insensitive).\n",
        "\n",
        "    Returns:\n",
        "      The identity matrix\n",
        "  '''\n",
        "  x_shortcut = x\n",
        "\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=stride, padding=padding )(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(rate=2e-1)(x)\n",
        "\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=stride, padding=padding )(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(rate=2e-1)(x)\n",
        "\n",
        "  x = Add()([x, x_shortcut])\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOtF4e1t4VOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_block(x, filters, kernel_size, stride, padding='same'):\n",
        "  '''\n",
        "    Creating a block of convolutinal layers with BatchNormalization and an activation of relu\n",
        "    \n",
        "    Arguments:\n",
        "      x           - Input tensor\n",
        "      filters     - Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "      kernel_size - An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
        "                    Can be a single integer to specify the same value for all spatial dimensions.\n",
        "      stride      - An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and \n",
        "                    width. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "      padding     - one of \"valid\" or \"same\" (case-insensitive).\n",
        "\n",
        "    Returns:\n",
        "      The identity matrix\n",
        "  '''\n",
        "  x = Conv2D(filters=filters, kernel_size=kernel_size, strides=stride, padding=padding)(x)\n",
        "  x = BatchNormalization(axis = 3)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(rate=2e-1)(x)\n",
        "  # x = AveragePooling2D(pool_size=(2,2), strides=stride, padding=padding)(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9blYNl2hZN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deconv_block(x, filters, kernel_size, stride, padding='same'):\n",
        "  '''\n",
        "  Creating a block of deconvolutinal layers with BatchNormalization and an activation of relu\n",
        "  \n",
        "  Arguments:\n",
        "    x           - Input tensor\n",
        "    filters     - Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "    kernel_size - An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
        "                  Can be a single integer to specify the same value for all spatial dimensions.\n",
        "    stride      - An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and \n",
        "                  width. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "    padding     - one of \"valid\" or \"same\" (case-insensitive).\n",
        "\n",
        "  Returns:\n",
        "    The identity matrix\n",
        "  '''\n",
        "  x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=stride, padding=padding)(x)\n",
        "  x = BatchNormalization(axis = 3)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Dropout(rate=2e-1)(x)\n",
        "\n",
        "  # x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=stride, padding=padding)(x)\n",
        "  # x = BatchNormalization(axis = 3)(x)\n",
        "  # x = Activation('relu')(x)\n",
        "  # x = Dropout(rate=2e-1)(x)\n",
        "  \n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7aNvZAQNtE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(input_shape=(512,512,3)):\n",
        "  '''\n",
        "    Creating the autoencoder as describes in the ReCoNet Paper:\n",
        "      https://arxiv.org/abs/1807.01197\n",
        "      \n",
        "    Arguments:\n",
        "      input_shape - tuple of integers, representing width, height and channels\n",
        "    \n",
        "    Returns:\n",
        "      model - an autoencoder arcutecture with 4 residual blocks\n",
        "  '''\n",
        "  x_input = Input(input_shape)\n",
        "\n",
        "  #####################################################################\n",
        "  # Encoder\n",
        "  #####################################################################\n",
        "  x = conv_block(x=x_input, filters=64 , kernel_size=2, stride=1, padding='same') \n",
        "  x = conv_block(x=x, filters=128, kernel_size=2, stride=2, padding='same')\n",
        "  x = conv_block(x=x, filters=256, kernel_size=2, stride=2, padding='same')\n",
        "  #####################################################################\n",
        "  # Encoder\n",
        "  #####################################################################\n",
        "\n",
        "\n",
        "\n",
        "  #####################################################################\n",
        "  # Residual Block\n",
        "  #####################################################################\n",
        "  x = identity_block(x=x, filters=256, stride=1, kernel_size=2)\n",
        "  x = identity_block(x=x, filters=256, stride=1, kernel_size=2)\n",
        "  x = identity_block(x=x, filters=256, stride=1, kernel_size=2)\n",
        "  x = identity_block(x=x, filters=256, stride=1, kernel_size=2)\n",
        "  #####################################################################\n",
        "  # Residual Block\n",
        "  #####################################################################\n",
        "\n",
        "\n",
        "\n",
        "  #####################################################################\n",
        "  # Encoder\n",
        "  #####################################################################\n",
        "  x = deconv_block(x=x, filters=256, kernel_size=2, stride=2, padding='same')\n",
        "  x = deconv_block(x=x, filters=128, kernel_size=2, stride=2, padding='same')\n",
        "  x = deconv_block(x=x, filters=64 , kernel_size=2, stride=1, padding='same')\n",
        "  #####################################################################\n",
        "  # Encoder\n",
        "  #####################################################################\n",
        "\n",
        "\n",
        "  x = Conv2DTranspose(filters=3, kernel_size=2, strides=1, padding='same')(x)\n",
        "  x = Activation('tanh')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs = x_input, outputs = x, name='myModel')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tDR0eeSUNrX3",
        "colab": {}
      },
      "source": [
        "model = create_model((512, 512, 3))\n",
        "tf.keras.utils.plot_model( model, to_file='model.png', show_shapes=True, show_layer_names=False, rankdir='TD', expand_nested=False, dpi=60 );"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qyi57BZ3Yeu",
        "colab_type": "text"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOos3D23q8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_layers = ['block4_conv2']   # Content VGG Layers\n",
        "style_layers   = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']   # Style VGG Layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWUd4qrHw8qc",
        "colab_type": "text"
      },
      "source": [
        "### VGG Feature Extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emYEUR5m3dZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG_FeatureExtractor(tf.keras.models.Model):\n",
        "\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super().__init__()\n",
        "    self.vgg =  self.get_vgg_layers(style_layers + content_layers)\n",
        "\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.num_content_layers = len(content_layers)\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    inputs = inputs*255.0\n",
        "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs) # pre processing the image using the VGG network\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n",
        "\n",
        "    style_outputs_gram = []\n",
        "    for style_output in style_outputs:\n",
        "      style_outputs_gram.append( self.gram_matrix(style_output) )\n",
        "\n",
        "    content_dict = {}\n",
        "    for content_name, value in zip ( self.content_layers, content_outputs ):\n",
        "      content_dict.update({content_name:value})\n",
        "\n",
        "    style_dict = {}\n",
        "    for style_name, value in zip( self.style_layers, style_outputs_gram ):\n",
        "      style_dict.update({style_name:value})\n",
        "    \n",
        "    return {'content':content_dict, 'style':style_dict}\n",
        "  \n",
        "  def get_vgg_layers(self, layer_names):\n",
        "\n",
        "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    \n",
        "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "    model = tf.keras.Model([vgg.input], outputs)\n",
        "    return model\n",
        "    \n",
        "  def gram_matrix(self, input_tensor):\n",
        "\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "    input_shape = tf.shape(input_tensor)\n",
        "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "    return result/(num_locations)\n",
        "\n",
        "  \n",
        "  def get_vgg_layers(self, layer_names):\n",
        "\n",
        "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    \n",
        "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "    model = tf.keras.Model([vgg.input], outputs)\n",
        "    return model\n",
        "    \n",
        "  def gram_matrix(self, input_tensor):\n",
        "\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "    input_shape = tf.shape(input_tensor)\n",
        "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "    return result/(num_locations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INaBiY8fu-cQ",
        "colab_type": "text"
      },
      "source": [
        "### Style Content loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDpWtCDrA73W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_content_loss(style_outputs, style_targets , content_outputs, content_targets):\n",
        "  style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()])\n",
        "  style_loss *= style_weight / len(style_layers)\n",
        "\n",
        "  content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()])\n",
        "  content_loss *= content_weight / len(content_layers)\n",
        "  \n",
        "  loss = style_loss + content_loss\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hz18YcwljWj",
        "colab_type": "text"
      },
      "source": [
        "## Prerequisite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUHkqNLmGtHT",
        "colab_type": "text"
      },
      "source": [
        "### Download content and style images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2q18w9RGscW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTENT_PATH  = tf.keras.utils.get_file('street.jpg',       'https://i.pinimg.com/originals/e3/f5/29/e3f529e7b972fc693f9a73b11401399a.jpg')\n",
        "STYLE_PATH    = tf.keras.utils.get_file('oilPainting_2.jpg','https://images-na.ssl-images-amazon.com/images/I/917exGYqGmL._SL1500_.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN3hhy0jG5z2",
        "colab_type": "text"
      },
      "source": [
        "### Image preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUA_HOU6w--L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coverting images to tensorflow readable tensors\n",
        "content_image = load_img(CONTENT_PATH)\n",
        "style_image   = load_img(STYLE_PATH)\n",
        "\n",
        "content_image = image_resize_with_crop_or_pad(content_image, 512, 512)\n",
        "style_image   = image_resize_with_crop_or_pad(style_image, 512, 512)\n",
        "\n",
        "# content_image = image_resize(content_image, 512)\n",
        "# style_image   = image_resize(style_image, 512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZUmDtmWHmRR",
        "colab_type": "text"
      },
      "source": [
        "### Style and content extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB6yiP49HsjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiating VGG feature extractor\n",
        "feature_extractor = VGG_FeatureExtractor(style_layers, content_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PtEsubR14-tu",
        "colab": {}
      },
      "source": [
        "content_targets = feature_extractor(content_image)[\"content\"]\n",
        "style_targets   = feature_extractor(style_image)[\"style\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbDVEb6Gl_f",
        "colab_type": "text"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywH4ktXeGpzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx1igt1uInUb",
        "colab_type": "text"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPeeMg8PImUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uClCSWFwRBYM",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xui7Wx_QG8gf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt    = tf.optimizers.Adam(learning_rate=1e-2, beta_1=0.99, epsilon=1e-1)\n",
        "image  = tf.Variable(content_image)\n",
        "output = tf.Variable(content_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDwAxfOkMKHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(image)\n",
        "    outputs = feature_extractor(prediction)\n",
        "  \n",
        "    loss  = style_content_loss(outputs[\"style\"], style_targets, outputs[\"content\"], content_targets)\n",
        "    loss += tf.image.total_variation(image) * total_variation_weight\n",
        "\n",
        "  grad = tape.gradient(loss, model.trainable_variables )\n",
        "  opt.apply_gradients(zip(grad, model.trainable_variables))\n",
        "  output.assign(clip_0_1(prediction))\n",
        "  train_loss(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APNKLkRLdgJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_path = \"/content/saved_images\"\n",
        "\n",
        "if not os.path.isdir(source_path):\n",
        "  os.mkdir(source_path)\n",
        "\n",
        "for epoch in tqdm.trange(1000):\n",
        "  train_step(image)\n",
        "\n",
        "  if epoch%10==0:\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(tensor_to_image(output))\n",
        "  if epoch%100==0:\n",
        "    img = tensor_to_image(output)\n",
        "    img.save( os.path.join(\"/content/saved_images\", get_time()+\".jpg\"))\n",
        "  \n",
        "    loss = train_loss.result()\n",
        "    print(\"train loss: {}\".format(loss.numpy()))\n",
        "  \n",
        "  train_loss.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n768dZTjKzVu",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PRLdTrgK5fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict(content_image)\n",
        "display.display( tensor_to_image( prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqWJ1ZaHYy5Z",
        "colab_type": "text"
      },
      "source": [
        "# Test coding cells\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIyOhKwb_CFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.optimizers.Adam(learning_rate=1e-2, beta_1=0.99, epsilon=1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8pSWtTM9SVkX",
        "colab": {}
      },
      "source": [
        "# model = tf.keras.Sequential([tf.keras.layers.Conv2D(128, 3,2,'same'),\n",
        "#                         tf.keras.layers.Conv2D(64, 3,2,'same'),\n",
        "#                         tf.keras.layers.Conv2D(32, 3,2,'same'),\n",
        "#                         tf.keras.layers.Conv2DTranspose(32, 3,2,'same'),\n",
        "#                         tf.keras.layers.Conv2DTranspose(64, 3,2,'same'),\n",
        "#                         tf.keras.layers.Conv2DTranspose(128, 3,2,'same'),\n",
        "#                         tf.keras.layers.Conv2DTranspose(3, 3,1,'same'),\n",
        "#                         ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_tD1cePHarR8",
        "colab": {}
      },
      "source": [
        "image = tf.Variable(content_image)\n",
        "output = tf.Variable(content_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veIYXUKr_P4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(image)\n",
        "    outputs = feature_extractor(prediction)\n",
        "  \n",
        "    loss  = style_content_loss(outputs[\"style\"], style_targets, outputs[\"content\"], content_targets)\n",
        "    loss += tf.image.total_variation(prediction)\n",
        "    # loss = tf.image.total_variation(image) * total_variation_weight\n",
        "\n",
        "\n",
        "  grad = tape.gradient(loss, model.trainable_variables )\n",
        "  opt.apply_gradients(zip(grad, model.trainable_variables))\n",
        "  output.assign(clip_0_1(prediction))\n",
        "  train_loss(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tX7xjZpcY0pu",
        "colab": {}
      },
      "source": [
        "source_path = \"/content/saved_images\"\n",
        "\n",
        "if not os.path.isdir(source_path):\n",
        "  os.mkdir(source_path)\n",
        "\n",
        "for epoch in tqdm.trange(10000):\n",
        "  train_step(output)\n",
        "\n",
        "  if epoch%100==0:\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(tensor_to_image(output))\n",
        "  # if epoch%100==0:\n",
        "  #   img = tensor_to_image(output)\n",
        "  #   img.save( os.path.join(\"/content/saved_images\", get_time()+\".jpg\"))\n",
        "  \n",
        "  #   loss = train_loss.result()\n",
        "  #   print(\"train loss: {}\".format(loss.numpy()))\n",
        "  \n",
        "  train_loss.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtcfYZMuxzYQ",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpXzJlDslRc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_video(\"./test.gif\", source_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MESw0lT69BYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_files(\"/content/saved_images\", \n",
        "             \"/content/model.png\", \n",
        "            #  \"/content/test.gif\"\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eadQqkfYkGwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}